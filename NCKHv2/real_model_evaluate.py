"""
Script Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh AI tháº­t cho phÃ¡t hiá»‡n tin giáº£
Sá»­ dá»¥ng model Vietnamese LLaMA vÃ  RAG system
TÃ¡c giáº£: [TÃªn cá»§a báº¡n]
NgÃ y táº¡o: 2024
"""

import pandas as pd
import numpy as np
from sklearn.metrics import precision_score, f1_score, mean_squared_error, confusion_matrix
import matplotlib.pyplot as plt
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer
import faiss
import re

# Set style cho biá»ƒu Ä‘á»“ Ä‘áº¹p hÆ¡n
plt.style.use('default')

# Load models (uncomment khi muá»‘n dÃ¹ng model tháº­t)
def load_models():
    """
    Load táº¥t cáº£ cÃ¡c models cáº§n thiáº¿t
    MÃ¬nh load má»™t láº§n Ä‘á»ƒ trÃ¡nh load láº¡i nhiá»u láº§n
    """
    print("Äang load cÃ¡c models...")
    
    # Load knowledge base
    with open("documents.txt", "r", encoding="utf-8") as f:
        documents = [line.strip() for line in f.readlines()]
    index = faiss.read_index("knowledge_base.index")
    
    # Load embedding model
    embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    
    # Load LLM - model Vietnamese LLaMA
    MODEL_NAME = "vilm/vinallama-7b"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    return documents, index, embedding_model, model, tokenizer

def extract_accuracy_from_response(response):
    """
    TrÃ­ch xuáº¥t Ä‘á»™ chÃ­nh xÃ¡c tá»« response cá»§a model
    MÃ¬nh dÃ¹ng regex Ä‘á»ƒ tÃ¬m sá»‘ pháº§n trÄƒm trong text
    """
    try:
        match = re.search(r"Äá»™ chÃ­nh xÃ¡c:.*?(\d+\.?\d*)", response, re.IGNORECASE)
        if match:
            return float(match.group(1)) / 100.0
        return 0.5  # Default náº¿u khÃ´ng tÃ¬m tháº¥y
    except:
        return 0.5

def predict_news_accuracy_real(news_snippet, documents, index, embedding_model, model, tokenizer):
    """
    Dá»± Ä‘oÃ¡n tháº­t báº±ng AI model
    Sá»­ dá»¥ng RAG system Ä‘á»ƒ tÃ¬m thÃ´ng tin liÃªn quan
    """
    k = 3  # Sá»‘ documents tÆ°Æ¡ng tá»± nháº¥t
    
    # Táº¡o embedding vÃ  tÃ¬m documents tÆ°Æ¡ng tá»±
    query_embedding = embedding_model.encode([news_snippet], convert_to_tensor=True).cpu().numpy().astype('float32')
    distances, indices = index.search(query_embedding, k)
    retrieved_docs = [documents[i] for i in indices[0]]
    context = "\n\n---\n\n".join(retrieved_docs)
    
    # Táº¡o prompt cho model
    prompt = f"""<s>[INST] <<SYS>>
Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn phÃ¢n tÃ­ch tin tá»©c tiáº¿ng Viá»‡t. Dá»±a vÃ o thÃ´ng tin trong Bá»I Cáº¢NH, hÃ£y phÃ¢n tÃ­ch TIN Tá»¨C Cáº¦N KIá»‚M TRA vÃ  Ä‘Æ°a ra cÃ¢u tráº£ lá»i gá»“m hai pháº§n: má»™t cÃ¢u phÃ¢n tÃ­ch ngáº¯n gá»n, vÃ  má»™t dÃ²ng riÃªng ghi "Äá»™ chÃ­nh xÃ¡c:" theo sau lÃ  má»™t con sá»‘ pháº§n trÄƒm.
<</SYS>>

Bá»I Cáº¢NH:
{context}

TIN Tá»¨C Cáº¦N KIá»‚M TRA:
{news_snippet} [/INST]
PHÃ‚N TÃCH:
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs, 
        max_new_tokens=256, 
        repetition_penalty=1.1,
        no_repeat_ngram_size=5,
        pad_token_id=tokenizer.eos_token_id
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    full_analysis = response.split("PHÃ‚N TÃCH:")[-1].strip()
    
    return extract_accuracy_from_response(full_analysis)

def mock_predict_accuracy(news_snippet):
    """
    HÃ m giáº£ láº­p dá»± Ä‘oÃ¡n Ä‘á»ƒ test nhanh
    """
    import random
    length_factor = min(len(news_snippet) / 500, 1.0)
    random_factor = random.random() * 0.3
    base_accuracy = 0.6 + (length_factor * 0.3) + random_factor
    return min(max(base_accuracy, 0.1), 0.95)

def plot_metrics(metrics_dict, save_path='evaluation_results.png'):
    """
    Táº¡o visualization toÃ n diá»‡n cho káº¿t quáº£ Ä‘Ã¡nh giÃ¡
    MÃ¬nh váº½ 4 biá»ƒu Ä‘á»“ khÃ¡c nhau Ä‘á»ƒ phÃ¢n tÃ­ch Ä‘áº§y Ä‘á»§
    """
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Káº¿t Quáº£ ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh PhÃ¡t Hiá»‡n Tin Giáº£', fontsize=16, fontweight='bold')
    
    # 1. Biá»ƒu Ä‘á»“ cá»™t cÃ¡c metrics
    metrics_names = ['Precision', 'F1-Score', 'Accuracy']
    metrics_values = [metrics_dict['precision'], metrics_dict['f1_score'], metrics_dict['accuracy']]
    colors = ['#2E86AB', '#A23B72', '#F18F01']
    
    bars = ax1.bar(metrics_names, metrics_values, color=colors, alpha=0.7)
    ax1.set_title('CÃ¡c Metrics Hiá»‡u Suáº¥t MÃ´ HÃ¬nh', fontweight='bold')
    ax1.set_ylabel('Äiá»ƒm sá»‘')
    ax1.set_ylim(0, 1)
    
    # ThÃªm giÃ¡ trá»‹ lÃªn cÃ¡c cá»™t
    for bar, value in zip(bars, metrics_values):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 2. Confusion Matrix
    cm = metrics_dict['confusion_matrix']
    im = ax2.imshow(cm, interpolation='nearest', cmap='Blues', aspect='auto')
    ax2.set_title('Ma Tráº­n Nháº§m Láº«n', fontweight='bold')
    
    # ThÃªm sá»‘ liá»‡u vÃ o matrix
    for i in range(2):
        for j in range(2):
            ax2.text(j, i, str(cm[i, j]), ha="center", va="center", 
                    color="white" if cm[i, j] > cm.max() / 2 else "black", 
                    fontweight='bold', fontsize=14)
    
    ax2.set_xticks([0, 1])
    ax2.set_yticks([0, 1])
    ax2.set_xticklabels(['Tháº­t', 'Giáº£'], fontweight='bold')
    ax2.set_yticklabels(['Tháº­t', 'Giáº£'], fontweight='bold')
    ax2.set_xlabel('Dá»± Ä‘oÃ¡n', fontweight='bold')
    ax2.set_ylabel('Thá»±c táº¿', fontweight='bold')
    
    # 3. Biá»ƒu Ä‘á»“ RMSE
    metrics_for_rmse = ['Precision', 'F1-Score', 'Accuracy']
    rmse_values = [metrics_dict['rmse']] * 3
    
    ax3.plot(metrics_for_rmse, rmse_values, 'ro-', linewidth=2, markersize=8)
    ax3.set_title('RMSE (CÃ ng tháº¥p cÃ ng tá»‘t)', fontweight='bold')
    ax3.set_ylabel('GiÃ¡ trá»‹ RMSE')
    ax3.text(1, metrics_dict['rmse'], f'RMSE: {metrics_dict["rmse"]:.3f}', 
             ha='center', va='bottom', fontweight='bold', fontsize=12,
             bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
    
    # 4. TÃ³m táº¯t hiá»‡u suáº¥t
    ax4.axis('off')
    summary_text = f"""
    ğŸ“Š TÃ“M Táº®T HIá»†U SUáº¤T MÃ” HÃŒNH
    
    ğŸ¯ Precision: {metrics_dict['precision']:.3f}
    âš¡ F1-Score: {metrics_dict['f1_score']:.3f}
    ğŸ“ˆ Accuracy: {metrics_dict['accuracy']:.3f}
    ğŸ“ RMSE: {metrics_dict['rmse']:.3f}
    
    ğŸ“‹ MA TRáº¬N NHáº¦M LáºªN:
    âœ… True Positives: {cm[0][0]}
    âŒ False Positives: {cm[1][0]}
    âœ… True Negatives: {cm[1][1]}
    âŒ False Negatives: {cm[0][1]}
    """
    
    ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, fontsize=12,
             verticalalignment='top', fontfamily='monospace', fontweight='bold',
             bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8, edgecolor='black'))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    return fig

def get_final_verdict(metrics_dict):
    """
    Táº¡o Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng dá»±a trÃªn cÃ¡c metrics
    MÃ¬nh dÃ¹ng weighted score Ä‘á»ƒ cÃ¢n báº±ng cÃ¡c metrics
    """
    precision = metrics_dict['precision']
    f1 = metrics_dict['f1_score']
    accuracy = metrics_dict['accuracy']
    rmse = metrics_dict['rmse']
    
    # TÃ­nh Ä‘iá»ƒm tá»•ng thá»ƒ (weighted average)
    overall_score = (precision * 0.3 + f1 * 0.3 + accuracy * 0.3 + (1 - rmse) * 0.1)
    
    # XÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ Ä‘Ã¡nh giÃ¡
    if overall_score >= 0.8:
        verdict = "XUáº¤T Sáº®C"
        grade = "A+"
        recommendation = "MÃ´ hÃ¬nh sáºµn sÃ ng cho production"
        emoji = "ğŸ‰"
    elif overall_score >= 0.7:
        verdict = "Tá»T"
        grade = "A"
        recommendation = "MÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t, cÃ³ thá»ƒ cáº£i thiá»‡n nhá»"
        emoji = "ğŸ‘"
    elif overall_score >= 0.6:
        verdict = "CHáº¤P NHáº¬N ÄÆ¯á»¢C"
        grade = "B"
        recommendation = "Cáº§n cáº£i thiá»‡n trÆ°á»›c khi deploy"
        emoji = "âœ…"
    elif overall_score >= 0.5:
        verdict = "Cáº¦N Cáº¢I THIá»†N"
        grade = "C"
        recommendation = "Cáº§n cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ"
        emoji = "âš ï¸"
    else:
        verdict = "KÃ‰M"
        grade = "D"
        recommendation = "Cáº§n overhaul hoÃ n toÃ n"
        emoji = "âŒ"
    
    return {
        'overall_score': overall_score,
        'verdict': verdict,
        'grade': grade,
        'recommendation': recommendation,
        'emoji': emoji
    }

def evaluate_model(use_real_model=False, sample_size=20):
    """
    ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh sá»­ dá»¥ng test dataset
    MÃ¬nh cho phÃ©p chá»n giá»¯a model tháº­t vÃ  mock Ä‘á»ƒ test
    """
    print("Äang load test dataset...")
    
    try:
        # Load dataset
        df = pd.read_csv('llm_training_complete.csv')
        print(f"Dataset Ä‘Ã£ load: {len(df)} dÃ²ng")
    except FileNotFoundError:
        print("KhÃ´ng tÃ¬m tháº¥y dataset. Táº¡o dá»¯ liá»‡u giáº£ Ä‘á»ƒ test...")
        np.random.seed(42)
        n_samples = 100
        df = pd.DataFrame({
            'content': [f"Tin tá»©c máº«u {i} vá»›i ná»™i dung chi tiáº¿t" * (i % 5 + 1) for i in range(n_samples)],
            'label': np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
        })
    
    # Lá»c bá» cÃ¡c dÃ²ng trá»‘ng
    df = df.dropna(subset=['content'])
    
    # Láº¥y máº«u Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
    sample_size = min(sample_size, len(df))
    df_sample = df.sample(n=sample_size, random_state=42)
    
    print(f"ÄÃ¡nh giÃ¡ trÃªn {len(df_sample)} máº«u...")
    
    # Load models náº¿u dÃ¹ng model tháº­t
    if use_real_model:
        print("Äang load AI models tháº­t...")
        documents, index, embedding_model, model, tokenizer = load_models()
        predict_func = lambda x: predict_news_accuracy_real(x, documents, index, embedding_model, model, tokenizer)
    else:
        print("DÃ¹ng mock predictions Ä‘á»ƒ test nhanh...")
        predict_func = mock_predict_accuracy
    
    # Chuáº©n bá»‹ dá»¯ liá»‡u
    X = df_sample['content'].tolist()
    y_true = df_sample['label'].tolist()
    
    # Láº¥y dá»± Ä‘oÃ¡n
    y_pred_proba = []
    y_pred_binary = []
    
    for i, news in enumerate(X):
        print(f"Äang xá»­ lÃ½ {i+1}/{len(X)}...")
        try:
            accuracy = predict_func(news)
            y_pred_proba.append(accuracy)
            y_pred_binary.append(0 if accuracy > 0.5 else 1)
        except Exception as e:
            print(f"Lá»—i xá»­ lÃ½ máº«u {i}: {e}")
            y_pred_proba.append(0.5)
            y_pred_binary.append(1)
    
    # TÃ­nh toÃ¡n metrics
    print("\n=== Káº¾T QUáº¢ ÄÃNH GIÃ ===")
    
    precision = precision_score(y_true, y_pred_binary, average='weighted')
    f1 = f1_score(y_true, y_pred_binary, average='weighted')
    rmse = np.sqrt(mean_squared_error(y_true, y_pred_proba))
    accuracy = np.mean(np.array(y_true) == np.array(y_pred_binary))
    
    print(f"Precision: {precision:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"Accuracy: {accuracy:.4f}")
    
    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred_binary)
    print(f"\nMa Tráº­n Nháº§m Láº«n:")
    print("      Dá»± Ä‘oÃ¡n")
    print("      Tháº­t  Giáº£")
    print(f"Tháº­t  {cm[0][0]:4d}  {cm[0][1]:4d}")
    print(f"Giáº£   {cm[1][0]:4d}  {cm[1][1]:4d}")
    
    # LÆ°u káº¿t quáº£
    results = {
        'precision': precision,
        'f1_score': f1,
        'rmse': rmse,
        'accuracy': accuracy,
        'confusion_matrix': cm
    }
    
    # Táº¡o Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng
    verdict = get_final_verdict(results)
    
    # In Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng
    print(f"\n{'='*50}")
    print(f"ğŸ¯ ÄÃNH GIÃ CUá»I CÃ™NG: {verdict['verdict']}")
    print(f"ğŸ“Š Äiá»ƒm tá»•ng thá»ƒ: {verdict['overall_score']:.3f}")
    print(f"ğŸ† Xáº¿p loáº¡i: {verdict['grade']}")
    print(f"ğŸ’¡ Khuyáº¿n nghá»‹: {verdict['recommendation']}")
    print(f"{'='*50}")
    
    # Táº¡o visualization
    print("\nğŸ“ˆ Äang táº¡o biá»ƒu Ä‘á»“...")
    plot_metrics(results)
    
    return results, verdict

if __name__ == "__main__":
    # Set use_real_model=True Ä‘á»ƒ test vá»›i AI model tháº­t
    # Set use_real_model=False Ä‘á»ƒ test nhanh vá»›i mock predictions
    results, verdict = evaluate_model(use_real_model=False, sample_size=20)
    
    print(f"\nğŸ“‹ TÃ“M Táº®T CUá»I CÃ™NG:")
    print(f"Precision: {results['precision']:.4f}")
    print(f"F1-Score: {results['f1_score']:.4f}")
    print(f"RMSE: {results['rmse']:.4f}")
    print(f"Accuracy: {results['accuracy']:.4f}")
    print(f"ÄÃ¡nh giÃ¡: {verdict['verdict']} ({verdict['grade']})") 