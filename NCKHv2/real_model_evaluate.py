"""
Script ƒë√°nh gi√° m√¥ h√¨nh AI th·∫≠t cho ph√°t hi·ªán tin gi·∫£
S·ª≠ d·ª•ng model Vietnamese LLaMA v√† RAG system
T√°c gi·∫£: [T√™n c·ªßa b·∫°n]
Ng√†y t·∫°o: 2024
"""

import pandas as pd
import numpy as np
from sklearn.metrics import precision_score, f1_score, mean_squared_error, confusion_matrix
import matplotlib.pyplot as plt
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer
import faiss
import re

# Set style cho bi·ªÉu ƒë·ªì ƒë·∫πp h∆°n
plt.style.use('default')

# Load models (uncomment khi mu·ªën d√πng model th·∫≠t)
def load_models():
    """
    Load t·∫•t c·∫£ c√°c models c·∫ßn thi·∫øt
    M√¨nh load m·ªôt l·∫ßn ƒë·ªÉ tr√°nh load l·∫°i nhi·ªÅu l·∫ßn
    """
    print("ƒêang load c√°c models...")
    
    # Load knowledge base
    with open("documents.txt", "r", encoding="utf-8") as f:
        documents = [line.strip() for line in f.readlines()]
    index = faiss.read_index("knowledge_base.index")
    
    # Load embedding model
    embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    
    # Load LLM - model Vietnamese LLaMA
    MODEL_NAME = "vilm/vinallama-7b"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    return documents, index, embedding_model, model, tokenizer

def extract_accuracy_from_response(response):
    """
    Tr√≠ch xu·∫•t ƒë·ªô ch√≠nh x√°c t·ª´ response c·ªßa model
    M√¨nh d√πng regex ƒë·ªÉ t√¨m s·ªë ph·∫ßn trƒÉm trong text
    """
    try:
        match = re.search(r"ƒê·ªô ch√≠nh x√°c:.*?(\d+\.?\d*)", response, re.IGNORECASE)
        if match:
            return float(match.group(1)) / 100.0
        return 0.5  # Default n·∫øu kh√¥ng t√¨m th·∫•y
    except:
        return 0.5

def predict_news_accuracy_real(news_snippet, documents, index, embedding_model, model, tokenizer):
    """
    D·ª± ƒëo√°n th·∫≠t b·∫±ng AI model
    S·ª≠ d·ª•ng RAG system ƒë·ªÉ t√¨m th√¥ng tin li√™n quan
    """
    k = 3  # S·ªë documents t∆∞∆°ng t·ª± nh·∫•t
    
    # T·∫°o embedding v√† t√¨m documents t∆∞∆°ng t·ª±
    query_embedding = embedding_model.encode([news_snippet], convert_to_tensor=True).cpu().numpy().astype('float32')
    distances, indices = index.search(query_embedding, k)
    retrieved_docs = [documents[i] for i in indices[0]]
    context = "\n\n---\n\n".join(retrieved_docs)
    
    # T·∫°o prompt cho model
    prompt = f"""<s>[INST] <<SYS>>
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n ph√¢n t√≠ch tin t·ª©c ti·∫øng Vi·ªát. D·ª±a v√†o th√¥ng tin trong B·ªêI C·∫¢NH, h√£y ph√¢n t√≠ch TIN T·ª®C C·∫¶N KI·ªÇM TRA v√† ƒë∆∞a ra c√¢u tr·∫£ l·ªùi g·ªìm hai ph·∫ßn: m·ªôt c√¢u ph√¢n t√≠ch ng·∫Øn g·ªçn, v√† m·ªôt d√≤ng ri√™ng ghi "ƒê·ªô ch√≠nh x√°c:" theo sau l√† m·ªôt con s·ªë ph·∫ßn trƒÉm.
<</SYS>>

B·ªêI C·∫¢NH:
{context}

TIN T·ª®C C·∫¶N KI·ªÇM TRA:
{news_snippet} [/INST]
PH√ÇN T√çCH:
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs, 
        max_new_tokens=256, 
        repetition_penalty=1.1,
        no_repeat_ngram_size=5,
        pad_token_id=tokenizer.eos_token_id
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    full_analysis = response.split("PH√ÇN T√çCH:")[-1].strip()
    
    return extract_accuracy_from_response(full_analysis)

def mock_predict_accuracy(news_snippet):
    """
    H√†m gi·∫£ l·∫≠p d·ª± ƒëo√°n ƒë·ªÉ test nhanh
    """
    import random
    length_factor = min(len(news_snippet) / 500, 1.0)
    random_factor = random.random() * 0.3
    base_accuracy = 0.6 + (length_factor * 0.3) + random_factor
    return min(max(base_accuracy, 0.1), 0.95)

def plot_metrics(metrics_dict, save_path='evaluation_results.png'):
    """
    T·∫°o visualization to√†n di·ªán cho k·∫øt qu·∫£ ƒë√°nh gi√°
    M√¨nh v·∫Ω 4 bi·ªÉu ƒë·ªì kh√°c nhau ƒë·ªÉ ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß
    """
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('K·∫øt Qu·∫£ ƒê√°nh Gi√° M√¥ H√¨nh Ph√°t Hi·ªán Tin Gi·∫£', fontsize=16, fontweight='bold')
    
    # 1. Bi·ªÉu ƒë·ªì c·ªôt c√°c metrics
    metrics_names = ['Precision', 'F1-Score', 'Accuracy']
    metrics_values = [metrics_dict['precision'], metrics_dict['f1_score'], metrics_dict['accuracy']]
    colors = ['#2E86AB', '#A23B72', '#F18F01']
    
    bars = ax1.bar(metrics_names, metrics_values, color=colors, alpha=0.7)
    ax1.set_title('C√°c Metrics Hi·ªáu Su·∫•t M√¥ H√¨nh', fontweight='bold')
    ax1.set_ylabel('ƒêi·ªÉm s·ªë')
    ax1.set_ylim(0, 1)
    
    # Th√™m gi√° tr·ªã l√™n c√°c c·ªôt
    for bar, value in zip(bars, metrics_values):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 2. Confusion Matrix
    cm = metrics_dict['confusion_matrix']
    im = ax2.imshow(cm, interpolation='nearest', cmap='Blues', aspect='auto')
    ax2.set_title('Ma Tr·∫≠n Nh·∫ßm L·∫´n', fontweight='bold')
    
    # Th√™m s·ªë li·ªáu v√†o matrix
    for i in range(2):
        for j in range(2):
            ax2.text(j, i, str(cm[i, j]), ha="center", va="center", 
                    color="white" if cm[i, j] > cm.max() / 2 else "black", 
                    fontweight='bold', fontsize=14)
    
    ax2.set_xticks([0, 1])
    ax2.set_yticks([0, 1])
    ax2.set_xticklabels(['Th·∫≠t', 'Gi·∫£'], fontweight='bold')
    ax2.set_yticklabels(['Th·∫≠t', 'Gi·∫£'], fontweight='bold')
    ax2.set_xlabel('D·ª± ƒëo√°n', fontweight='bold')
    ax2.set_ylabel('Th·ª±c t·∫ø', fontweight='bold')
    
    # 3. Bi·ªÉu ƒë·ªì RMSE
    metrics_for_rmse = ['Precision', 'F1-Score', 'Accuracy']
    rmse_values = [metrics_dict['rmse']] * 3
    
    ax3.plot(metrics_for_rmse, rmse_values, 'ro-', linewidth=2, markersize=8)
    ax3.set_title('RMSE (C√†ng th·∫•p c√†ng t·ªët)', fontweight='bold')
    ax3.set_ylabel('Gi√° tr·ªã RMSE')
    ax3.text(1, metrics_dict['rmse'], f'RMSE: {metrics_dict["rmse"]:.3f}', 
             ha='center', va='bottom', fontweight='bold', fontsize=12,
             bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
    
    # 4. T√≥m t·∫Øt hi·ªáu su·∫•t
    ax4.axis('off')
    summary_text = f"""
    üìä T√ìM T·∫ÆT HI·ªÜU SU·∫§T M√î H√åNH
    
    üéØ Precision: {metrics_dict['precision']:.3f}
    ‚ö° F1-Score: {metrics_dict['f1_score']:.3f}
    üìà Accuracy: {metrics_dict['accuracy']:.3f}
    üìè RMSE: {metrics_dict['rmse']:.3f}
    
    üìã MA TR·∫¨N NH·∫¶M L·∫™N:
    ‚úÖ True Positives: {cm[0][0]}
    ‚ùå False Positives: {cm[1][0]}
    ‚úÖ True Negatives: {cm[1][1]}
    ‚ùå False Negatives: {cm[0][1]}
    """
    
    ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, fontsize=12,
             verticalalignment='top', fontfamily='monospace', fontweight='bold',
             bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8, edgecolor='black'))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    return fig

def get_final_verdict(metrics_dict):
    """
    T·∫°o ƒë√°nh gi√° cu·ªëi c√πng d·ª±a tr√™n c√°c metrics
    M√¨nh d√πng weighted score ƒë·ªÉ c√¢n b·∫±ng c√°c metrics
    """
    precision = metrics_dict['precision']
    f1 = metrics_dict['f1_score']
    accuracy = metrics_dict['accuracy']
    rmse = metrics_dict['rmse']
    
    # T√≠nh ƒëi·ªÉm t·ªïng th·ªÉ (weighted average)
    overall_score = (precision * 0.3 + f1 * 0.3 + accuracy * 0.3 + (1 - rmse) * 0.1)
    
    # X√°c ƒë·ªãnh m·ª©c ƒë·ªô ƒë√°nh gi√°
    if overall_score >= 0.8:
        verdict = "XU·∫§T S·∫ÆC"
        grade = "A+"
        recommendation = "M√¥ h√¨nh s·∫µn s√†ng cho production"
        emoji = "üéâ"
    elif overall_score >= 0.7:
        verdict = "T·ªêT"
        grade = "A"
        recommendation = "M√¥ h√¨nh ho·∫°t ƒë·ªông t·ªët, c√≥ th·ªÉ c·∫£i thi·ªán nh·ªè"
        emoji = "üëç"
    elif overall_score >= 0.6:
        verdict = "CH·∫§P NH·∫¨N ƒê∆Ø·ª¢C"
        grade = "B"
        recommendation = "C·∫ßn c·∫£i thi·ªán tr∆∞·ªõc khi deploy"
        emoji = "‚úÖ"
    elif overall_score >= 0.5:
        verdict = "C·∫¶N C·∫¢I THI·ªÜN"
        grade = "C"
        recommendation = "C·∫ßn c·∫£i thi·ªán ƒë√°ng k·ªÉ"
        emoji = "‚ö†Ô∏è"
    else:
        verdict = "K√âM"
        grade = "D"
        recommendation = "C·∫ßn overhaul ho√†n to√†n"
        emoji = "‚ùå"
    
    return {
        'overall_score': overall_score,
        'verdict': verdict,
        'grade': grade,
        'recommendation': recommendation,
        'emoji': emoji
    }

def evaluate_model(use_real_model=False, sample_size=20):
    """
    ƒê√°nh gi√° m√¥ h√¨nh s·ª≠ d·ª•ng test dataset
    M√¨nh cho ph√©p ch·ªçn gi·ªØa model th·∫≠t v√† mock ƒë·ªÉ test
    """
    print("ƒêang load test dataset...")
    
    try:
        # Load dataset
        df = pd.read_csv('llm_training_complete.csv')
        print(f"Dataset ƒë√£ load: {len(df)} d√≤ng")
    except FileNotFoundError:
        print("Kh√¥ng t√¨m th·∫•y dataset. T·∫°o d·ªØ li·ªáu gi·∫£ ƒë·ªÉ test...")
        np.random.seed(42)
        n_samples = 100
        df = pd.DataFrame({
            'content': [f"Tin t·ª©c m·∫´u {i} v·ªõi n·ªôi dung chi ti·∫øt" * (i % 5 + 1) for i in range(n_samples)],
            'label': np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
        })
    
    # L·ªçc b·ªè c√°c d√≤ng tr·ªëng
    df = df.dropna(subset=['content'])
    
    # L·∫•y m·∫´u ƒë·ªÉ ƒë√°nh gi√°
    sample_size = min(sample_size, len(df))
    df_sample = df.sample(n=sample_size, random_state=42)
    
    print(f"ƒê√°nh gi√° tr√™n {len(df_sample)} m·∫´u...")
    
    # Load models n·∫øu d√πng model th·∫≠t
    if use_real_model:
        print("ƒêang load AI models th·∫≠t...")
        documents, index, embedding_model, model, tokenizer = load_models()
        predict_func = lambda x: predict_news_accuracy_real(x, documents, index, embedding_model, model, tokenizer)
    else:
        print("D√πng mock predictions ƒë·ªÉ test nhanh...")
        predict_func = mock_predict_accuracy
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu
    X = df_sample['content'].tolist()
    y_true = df_sample['label'].tolist()
    
    # L·∫•y d·ª± ƒëo√°n
    y_pred_proba = []
    y_pred_binary = []
    
    for i, news in enumerate(X):
        print(f"ƒêang x·ª≠ l√Ω {i+1}/{len(X)}...")
        try:
            accuracy = predict_func(news)
            y_pred_proba.append(accuracy)
            y_pred_binary.append(0 if accuracy > 0.5 else 1)
        except Exception as e:
            print(f"L·ªói x·ª≠ l√Ω m·∫´u {i}: {e}")
            y_pred_proba.append(0.5)
            y_pred_binary.append(1)
    
    # T√≠nh to√°n metrics
    print("\n=== K·∫æT QU·∫¢ ƒê√ÅNH GI√Å ===")
    
    precision = precision_score(y_true, y_pred_binary, average='weighted')
    f1 = f1_score(y_true, y_pred_binary, average='weighted')
    rmse = np.sqrt(mean_squared_error(y_true, y_pred_proba))
    accuracy = np.mean(np.array(y_true) == np.array(y_pred_binary))
    
    print(f"Precision: {precision:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"Accuracy: {accuracy:.4f}")
    
    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred_binary)
    print(f"\nMa Tr·∫≠n Nh·∫ßm L·∫´n:")
    print("      D·ª± ƒëo√°n")
    print("      Th·∫≠t  Gi·∫£")
    print(f"Th·∫≠t  {cm[0][0]:4d}  {cm[0][1]:4d}")
    print(f"Gi·∫£   {cm[1][0]:4d}  {cm[1][1]:4d}")
    
    # L∆∞u k·∫øt qu·∫£
    results = {
        'precision': precision,
        'f1_score': f1,
        'rmse': rmse,
        'accuracy': accuracy,
        'confusion_matrix': cm
    }
    
    # T·∫°o ƒë√°nh gi√° cu·ªëi c√πng
    verdict = get_final_verdict(results)
    
    # In ƒë√°nh gi√° cu·ªëi c√πng
    print(f"\n{'='*50}")
    print(f"üéØ ƒê√ÅNH GI√Å CU·ªêI C√ôNG: {verdict['verdict']}")
    print(f"üìä ƒêi·ªÉm t·ªïng th·ªÉ: {verdict['overall_score']:.3f}")
    print(f"üèÜ X·∫øp lo·∫°i: {verdict['grade']}")
    print(f"üí° Khuy·∫øn ngh·ªã: {verdict['recommendation']}")
    print(f"{'='*50}")
    
    # T·∫°o visualization
    print("\nüìà ƒêang t·∫°o bi·ªÉu ƒë·ªì...")
    plot_metrics(results)
    
    return results, verdict

if __name__ == "__main__":
    # Set use_real_model=True ƒë·ªÉ test v·ªõi AI model th·∫≠t
    # Set use_real_model=False ƒë·ªÉ test nhanh v·ªõi mock predictions
    results, verdict = evaluate_model(use_real_model=False, sample_size=20)
    
    print(f"\nüìã T√ìM T·∫ÆT CU·ªêI C√ôNG:")
    print(f"Precision: {results['precision']:.4f}")
    print(f"F1-Score: {results['f1_score']:.4f}")
    print(f"RMSE: {results['rmse']:.4f}")
    print(f"Accuracy: {results['accuracy']:.4f}")
    print(f"ƒê√°nh gi√°: {verdict['verdict']} ({verdict['grade']})") 