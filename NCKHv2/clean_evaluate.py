"""
Script Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh phÃ¡t hiá»‡n tin giáº£
Sá»­ dá»¥ng cÃ¡c metrics chuáº©n: Precision, F1-Score, RMSE, vÃ  Accuracy
TÃ¡c giáº£: NgÃµng LuyÃªn
NgÃ y táº¡o: 25/08/2025
"""
#Imprt thÆ° viá»‡nn
import pandas as pd
import numpy as np
from sklearn.metrics import precision_score, f1_score, mean_squared_error, confusion_matrix
import matplotlib.pyplot as plt
import random
import warnings

# Ignore warnings
warnings.filterwarnings('ignore')

class ModelEvaluator:
    """
    Class Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh vá»›i cÃ¡c metrics toÃ n diá»‡n vÃ  visualization
    MÃ¬nh viáº¿t class nÃ y Ä‘á»ƒ code dá»… quáº£n lÃ½ vÃ  má»Ÿ rá»™ng hÆ¡n
    """
    
    def __init__(self, use_mock=True):
        self.use_mock = use_mock
        self.results = {}
        self.verdict = {}
        
    def mock_predict_accuracy(self, news_snippet):
        """
        HÃ m giáº£ láº­p dá»± Ä‘oÃ¡n Ä‘á»ƒ test nhanh
        Trong thá»±c táº¿ sáº½ thay báº±ng model AI tháº­t
        """
        # Giáº£ láº­p dá»±a trÃªn Ä‘á»™ dÃ i text vÃ  má»™t chÃºt randomness
        length_factor = min(len(news_snippet) / 500, 1.0)
        random_factor = random.random() * 0.3
        base_accuracy = 0.6 + (length_factor * 0.3) + random_factor
        return min(max(base_accuracy, 0.1), 0.95)  # Giá»›i háº¡n tá»« 0.1 Ä‘áº¿n 0.95
    
    def load_data(self, file_path='llm_training_complete.csv'):
        """
        Load vÃ  chuáº©n bá»‹ dataset Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
        Náº¿u khÃ´ng tÃ¬m tháº¥y file thÃ¬ táº¡o data giáº£ Ä‘á»ƒ test
        """
        try:
            df = pd.read_csv(file_path)
            print(f"âœ… ÄÃ£ load dataset: {len(df)} dÃ²ng dá»¯ liá»‡u")
            return df
        except FileNotFoundError:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y dataset. Táº¡o dá»¯ liá»‡u giáº£ Ä‘á»ƒ test...")
            return self.create_synthetic_data()
    
    def create_synthetic_data(self, n_samples=100):
        """
        Táº¡o dá»¯ liá»‡u giáº£ Ä‘á»ƒ test khi khÃ´ng cÃ³ dataset tháº­t
        """
        np.random.seed(42)  # Äá»ƒ káº¿t quáº£ reproducible
        df = pd.DataFrame({
            'content': [f"Tin tá»©c máº«u {i} vá»›i ná»™i dung chi tiáº¿t" * (i % 5 + 1) for i in range(n_samples)],
            'label': np.random.choice([0, 1], n_samples, p=[0.7, 0.3])  # 70% tin tháº­t, 30% tin giáº£
        })
        return df
    
    def calculate_metrics(self, y_true, y_pred_binary, y_pred_proba):
        """
        TÃ­nh toÃ¡n táº¥t cáº£ cÃ¡c metrics Ä‘Ã¡nh giÃ¡
        MÃ¬nh dÃ¹ng weighted average Ä‘á»ƒ xá»­ lÃ½ imbalanced data
        """
        metrics = {}
        
        # CÃ¡c metrics cÆ¡ báº£n
        metrics['precision'] = precision_score(y_true, y_pred_binary, average='weighted', zero_division=0)
        metrics['f1_score'] = f1_score(y_true, y_pred_binary, average='weighted', zero_division=0)
        metrics['accuracy'] = np.mean(np.array(y_true) == np.array(y_pred_binary))
        metrics['rmse'] = np.sqrt(mean_squared_error(y_true, y_pred_proba))
        
        # Confusion matrix Ä‘á»ƒ phÃ¢n tÃ­ch chi tiáº¿t
        metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred_binary)
        
        return metrics
    
    def generate_verdict(self, metrics):
        """
        Táº¡o Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng dá»±a trÃªn cÃ¡c metrics
        MÃ¬nh dÃ¹ng weighted score Ä‘á»ƒ cÃ¢n báº±ng cÃ¡c metrics khÃ¡c nhau
        """
        precision = metrics['precision']
        f1 = metrics['f1_score']
        accuracy = metrics['accuracy']
        rmse = metrics['rmse']
        
        # TÃ­nh Ä‘iá»ƒm tá»•ng thá»ƒ (weighted average)
        overall_score = (precision * 0.3 + f1 * 0.3 + accuracy * 0.3 + (1 - rmse) * 0.1)
        
        # XÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ Ä‘Ã¡nh giÃ¡
        if overall_score >= 0.8:
            verdict = "XUáº¤T Sáº®C"
            grade = "A+"
            recommendation = "MÃ´ hÃ¬nh sáºµn sÃ ng cho production"
            emoji = "ğŸ‰"
        elif overall_score >= 0.7:
            verdict = "Tá»T"
            grade = "A"
            recommendation = "MÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t, cÃ³ thá»ƒ cáº£i thiá»‡n nhá»"
            emoji = "ğŸ‘"
        elif overall_score >= 0.6:
            verdict = "CHáº¤P NHáº¬N ÄÆ¯á»¢C"
            grade = "B"
            recommendation = "Cáº§n cáº£i thiá»‡n trÆ°á»›c khi deploy"
            emoji = "âœ…"
        elif overall_score >= 0.5:
            verdict = "Cáº¦N Cáº¢I THIá»†N"
            grade = "C"
            recommendation = "Cáº§n cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ"
            emoji = "âš ï¸"
        else:
            verdict = "KÃ‰M"
            grade = "D"
            recommendation = "Cáº§n overhaul hoÃ n toÃ n"
            emoji = "âŒ"
        
        return {
            'overall_score': overall_score,
            'verdict': verdict,
            'grade': grade,
            'recommendation': recommendation,
            'emoji': emoji
        }
    
    def create_visualizations(self, metrics, save_path='evaluation_results.png'):
        """
        Táº¡o dashboard visualization toÃ n diá»‡n
        MÃ¬nh dÃ¹ng matplotlib Ä‘á»ƒ váº½ 4 biá»ƒu Ä‘á»“ khÃ¡c nhau
        """
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Dashboard ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh PhÃ¡t Hiá»‡n Tin Giáº£', fontsize=18, fontweight='bold')
        
        # 1. Biá»ƒu Ä‘á»“ cá»™t cÃ¡c metrics
        metrics_names = ['Precision', 'F1-Score', 'Accuracy']
        metrics_values = [metrics['precision'], metrics['f1_score'], metrics['accuracy']]
        colors = ['#2E86AB', '#A23B72', '#F18F01']  # MÃ u Ä‘áº¹p cho biá»ƒu Ä‘á»“
        
        bars = ax1.bar(metrics_names, metrics_values, color=colors, alpha=0.8, edgecolor='black', linewidth=1)
        ax1.set_title('CÃ¡c Metrics Hiá»‡u Suáº¥t MÃ´ HÃ¬nh', fontweight='bold', fontsize=14)
        ax1.set_ylabel('Äiá»ƒm sá»‘', fontweight='bold')
        ax1.set_ylim(0, 1)
        ax1.grid(axis='y', alpha=0.3)
        
        # ThÃªm giÃ¡ trá»‹ lÃªn cÃ¡c cá»™t
        for bar, value in zip(bars, metrics_values):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=12)
        
        # 2. Confusion Matrix
        cm = metrics['confusion_matrix']
        im = ax2.imshow(cm, interpolation='nearest', cmap='Blues', aspect='auto')
        ax2.set_title('Ma Tráº­n Nháº§m Láº«n', fontweight='bold', fontsize=14)
        
        # ThÃªm sá»‘ liá»‡u vÃ o matrix
        for i in range(2):
            for j in range(2):
                ax2.text(j, i, str(cm[i, j]), ha="center", va="center", 
                        color="white" if cm[i, j] > cm.max() / 2 else "black", 
                        fontweight='bold', fontsize=14)
        
        ax2.set_xticks([0, 1])
        ax2.set_yticks([0, 1])
        ax2.set_xticklabels(['Tháº­t', 'Giáº£'], fontweight='bold')
        ax2.set_yticklabels(['Tháº­t', 'Giáº£'], fontweight='bold')
        ax2.set_xlabel('Dá»± Ä‘oÃ¡n', fontweight='bold')
        ax2.set_ylabel('Thá»±c táº¿', fontweight='bold')
        
        # 3. Biá»ƒu Ä‘á»“ RMSE
        ax3.plot(['Precision', 'F1-Score', 'Accuracy'], [metrics['rmse']] * 3, 
                'ro-', linewidth=3, markersize=10, markerfacecolor='red', markeredgecolor='black')
        ax3.set_title('RMSE (CÃ ng tháº¥p cÃ ng tá»‘t)', fontweight='bold', fontsize=14)
        ax3.set_ylabel('GiÃ¡ trá»‹ RMSE', fontweight='bold')
        ax3.grid(axis='y', alpha=0.3)
        ax3.text(1, metrics['rmse'], f'RMSE: {metrics["rmse"]:.3f}', 
                ha='center', va='bottom', fontweight='bold', fontsize=12,
                bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
        
        # 4. TÃ³m táº¯t hiá»‡u suáº¥t
        ax4.axis('off')
        summary_text = f"""
        ğŸ“Š TÃ“M Táº®T HIá»†U SUáº¤T MÃ” HÃŒNH
        
        ğŸ¯ Precision: {metrics['precision']:.3f}
        âš¡ F1-Score: {metrics['f1_score']:.3f}
        ğŸ“ˆ Accuracy: {metrics['accuracy']:.3f}
        ğŸ“ RMSE: {metrics['rmse']:.3f}
        
        ğŸ“‹ MA TRáº¬N NHáº¦M LáºªN:
        âœ… True Positives: {cm[0][0]}
        âŒ False Positives: {cm[1][0]}
        âœ… True Negatives: {cm[1][1]}
        âŒ False Negatives: {cm[0][1]}
        """
        
        ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=12,
                verticalalignment='top', fontfamily='monospace', fontweight='bold',
                bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8, edgecolor='black'))
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.show()
        
        return fig
    
    def print_results(self, metrics, verdict):
        """
        In káº¿t quáº£ Ä‘Ã¡nh giÃ¡ ra console
        Format Ä‘áº¹p vÃ  dá»… Ä‘á»c
        """
        print("\n" + "="*70)
        print("ğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ")
        print("="*70)
        print(f"ğŸ¯ Precision:    {metrics['precision']:.4f}")
        print(f"âš¡ F1-Score:     {metrics['f1_score']:.4f}")
        print(f"ğŸ“ˆ Accuracy:     {metrics['accuracy']:.4f}")
        print(f"ğŸ“ RMSE:         {metrics['rmse']:.4f}")
        
        cm = metrics['confusion_matrix']
        print(f"\nğŸ“‹ Ma Tráº­n Nháº§m Láº«n:")
        print("      Dá»± Ä‘oÃ¡n")
        print("      Tháº­t  Giáº£")
        print(f"Tháº­t  {cm[0][0]:4d}  {cm[0][1]:4d}")
        print(f"Giáº£   {cm[1][0]:4d}  {cm[1][1]:4d}")
        
        print(f"\n{verdict['emoji']} ÄÃNH GIÃ CUá»I CÃ™NG: {verdict['verdict']}")
        print(f"ğŸ“Š Äiá»ƒm tá»•ng thá»ƒ: {verdict['overall_score']:.3f}")
        print(f"ğŸ† Xáº¿p loáº¡i: {verdict['grade']}")
        print(f"ğŸ’¡ Khuyáº¿n nghá»‹: {verdict['recommendation']}")
        print("="*70)
    
    def evaluate(self, sample_size=30):
        """
        HÃ m chÃ­nh Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh
        MÃ¬nh giá»›i háº¡n sample size Ä‘á»ƒ cháº¡y nhanh, cÃ³ thá»ƒ Ä‘iá»u chá»‰nh
        """
        print("ğŸš€ Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh...")
        
        # Load dá»¯ liá»‡u
        df = self.load_data()
        df = df.dropna(subset=['content'])  # Bá» cÃ¡c dÃ²ng trá»‘ng
        
        # Láº¥y máº«u Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
        sample_size = min(sample_size, len(df))
        df_sample = df.sample(n=sample_size, random_state=42)  # Äá»ƒ reproducible
        print(f"ğŸ“ ÄÃ¡nh giÃ¡ trÃªn {len(df_sample)} máº«u...")
        
        # Chuáº©n bá»‹ dá»¯ liá»‡u
        X = df_sample['content'].tolist()
        y_true = df_sample['label'].tolist()  # 0 = tháº­t, 1 = giáº£
        
        # Láº¥y dá»± Ä‘oÃ¡n
        y_pred_proba = []
        y_pred_binary = []
        
        for i, news in enumerate(X, 1):
            print(f"Äang xá»­ lÃ½ {i}/{len(X)}...", end='\r')
            try:
                accuracy = self.mock_predict_accuracy(news)
                y_pred_proba.append(accuracy)
                # Chuyá»ƒn thÃ nh binary: accuracy > 0.5 = tháº­t (0), <= 0.5 = giáº£ (1)
                y_pred_binary.append(0 if accuracy > 0.5 else 1)
            except Exception as e:
                print(f"\nLá»—i xá»­ lÃ½ máº«u {i}: {e}")
                y_pred_proba.append(0.5)
                y_pred_binary.append(1)
        
        print(f"\nâœ… HoÃ n thÃ nh xá»­ lÃ½!")
        
        # TÃ­nh toÃ¡n metrics
        self.results = self.calculate_metrics(y_true, y_pred_binary, y_pred_proba)
        
        # Táº¡o Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng
        self.verdict = self.generate_verdict(self.results)
        
        # In káº¿t quáº£
        self.print_results(self.results, self.verdict)
        
        # Táº¡o visualization
        print("\nğŸ“ˆ Äang táº¡o biá»ƒu Ä‘á»“...")
        self.create_visualizations(self.results)
        
        return self.results, self.verdict

def main():
    """
    HÃ m main Ä‘á»ƒ cháº¡y Ä‘Ã¡nh giÃ¡
    MÃ¬nh dÃ¹ng mock predictions Ä‘á»ƒ test nhanh
    """
    evaluator = ModelEvaluator(use_mock=True)
    results, verdict = evaluator.evaluate(sample_size=30)
    
    print(f"\nğŸ“‹ TÃ“M Táº®T CUá»I CÃ™NG:")
    print(f"Precision: {results['precision']:.4f}")
    print(f"F1-Score: {results['f1_score']:.4f}")
    print(f"RMSE: {results['rmse']:.4f}")
    print(f"Accuracy: {results['accuracy']:.4f}")
    print(f"ÄÃ¡nh giÃ¡: {verdict['verdict']} ({verdict['grade']}) {verdict['emoji']}")

if __name__ == "__main__":
    main() 